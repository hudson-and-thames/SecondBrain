# Mutual information


---
Topics :: [[Machine Learning]]
Reference :: [Baudot, P., Tapia, M., Bennequin, D. and Goaillard, J.M., 2019. Topological information data analysis. _Entropy_, _21_(9), p.869.](https://www.mdpi.com/1099-4300/21/9/869/htm)
Type :: #atom
Creator :: Michelle
Discussion ::
Dis_Topic :: 
Date :: 2022-07-15 09:05


Mutual information (MI) forms part of information theory. Therefore, estimates of the MI are fundamentally important in most information theory applications. Mutual information is calculated between two variables and measures the reduction in uncertainty for one variable given a known value of the other variable.  The concept of mutual information is intimately linked to that of [entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory) "Entropy (information theory)") of a random variable, a fundamental notion in information theory that quantifies the expected "amount of information" held in a random variable. Variables are not always independent from each other. The dependence between variables increases redundant information. Determines how different the [joint distribution](https://en.wikipedia.org/wiki/Joint_distribution "Joint distribution") of a pair of variables is from the product of the marginal distributions of each variable.

Mutual information has been frequently used to perform feature selection in machine learning. For a given feature, we can measure the featureâ€™s mutual information with the class labels. If the mutual information is high, then the feature is a strong indicator of the class.