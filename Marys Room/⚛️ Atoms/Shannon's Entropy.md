# Shannon's Entropy
The “Shannon entropy” is a concept introduced by Shannon (1948), where a measure of the uncertainty of occurrence of certain event, given partial information about the system, is proposed.

Understanding Shannon’s entropy is crucial for [[Machine Learning]]. Shannon’s Entropy leads to a function which is the bread and butter of an ML practitioner —the [[Cross Entropy]] that is heavily used as a loss function in classification and also the [[KL divergence]] which is widely used in variational inference

 The **entropy of a variable** is the “amount of information” contained in the variable.


---
Topics :: [[Information theory]] 
Reference :: [Aerin Kim](https://towardsdatascience.com/the-intuition-behind-shannons-entropy-e74820fe9800)
Type :: #atom
Creator :: Michael 
TAF ::
Discussion ::
Dis_Topic :: 
Resolved ::
Date :: 2022-07-15 08:49
